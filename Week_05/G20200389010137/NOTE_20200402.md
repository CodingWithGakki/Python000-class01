# 舆情监控

## jieba

匹配精确度

歧义切分

未登录词的识别

训练的语料

### 分词

分词算法：常用词字典、词库；基于词频统计

pip install jieba

```python
import jieba
jieba.enable_paddle() # 0.4版本以后支持paddle模式
strings = ['我来自极客大学', 'Python进阶训练营真好玩']

for string in strings:
    result = jieba.cut(string, cut_all=False) # 精确模式
    print('Default Mode: ' + '/'.join(list(result)))

for string in strings:
    result = jieba.cut(string, cut_all=True) # 全模式
    print('Full Mode: ' + '/'.join(list(result)))


for string in strings:
    result = jieba.cut(string, use_paddle=True) # paddle模式
    print('Paddle Mode: ' + '/'.join(list(result)))

result = jieba.cut('钟南山院士接受采访新冠不会二次暴发') # 默认是精确模式
print('/'.join(list(result)))
# "新冠" 没有在词典中，但是被Viterbi算法识别出来了

result = jieba.cut_for_search('小明硕士毕业于中国科学院计算所，后在日本京都大学深造') # 搜索引擎模式
print('Search Mode: ' + '/'.join(list(result)))
```

### 提取关键字

```python
import jieba.analyse

text = '机器学习，需要一定的数学基础，需要掌握的数学基础知识特别多，如果从头到尾开始学，估计大部分人来不及，我建议先学习最基础的数学知识'

# 基于TF-IDF算法进行关键词抽取，推荐用，更贴合人类的用途
tfidf = jieba.analyse.extract_tags(text,
    topK=5,                   # 权重最大的topK个关键词
    withWeight=False)         # 返回每个关键字的权重值

# 基于TextRank算法进行关键词抽取
textrank = jieba.analyse.textrank(text,
    topK=5,                   # 权重最大的topK个关键词
    withWeight=False)         # 返回每个关键字的权重值

import pprint             # pprint 模块提供了打印出任何Python数据结构的类和方法
pprint.pprint(tfidf)
pprint.pprint(textrank)
```

### 停止词

要指定过滤的词放到一个文件中，每行一个

也是用 TF-IDF 算法

```python
import jieba
import jieba.analyse

text = '机器学习，需要一定的数学基础，需要掌握的数学基础知识特别多，如果从头到尾开始学，估计大部分人来不及，我建议先学习最基础的数学知识'
# stop_words 的文件格式是文本文件，每行一个词语
stop_words=r'../extra_dict/stop_words.txt'

jieba.analyse.set_stop_words(stop_words)

textrank = jieba.analyse.textrank(text,
    topK=5,
    withWeight=False)

import pprint             # pprint 模块提供了打印出任何Python数据结构的类和方法
pprint.pprint(textrank)
```

## 自定义词典

调整

### 静态

```python
import jieba

string = '极客大学Python进阶训练营真好玩'
user_dict=r'day0402/extra_dict/user_dict.txt'

# 自定义词典
jieba.load_userdict(user_dict)

result = jieba.cut(string, cut_all=False)
print('自定义: ' + '/'.join(list(result)))

print('=' * 40 )
```

cat stop_words.txt：

```txt
# 3：权重。可省略，有多个的时候可以设置权重
# nt：词性。可省略。nt 是名词，vd 动词
Python进阶训练营 3 nt
```

词性表：

```txt
# 词性表

# 1. 名词 (1个一类，7个二类，5个三类)
# 名词分为以下子类：
# n 名词
# nr 人名
# nr1 汉语姓氏
# nr2 汉语名字
# nrj 日语人名
# nrf 音译人名
# ns 地名
# nsf 音译地名
# nt 机构团体名
# nz 其它专名
# nl 名词性惯用语
# ng 名词性语素
# 2. 时间词(1个一类，1个二类)
# t 时间词
# tg 时间词性语素
# 3. 处所词(1个一类)
# s 处所词
# 4. 方位词(1个一类)
# f 方位词
# 5. 动词(1个一类，9个二类)
# v 动词
# vd 副动词
# vn 名动词
# vshi 动词“是”
# vyou 动词“有”
# vf 趋向动词
# vx 形式动词
# vi 不及物动词（内动词）
# vl 动词性惯用语
# vg 动词性语素
# 6. 形容词(1个一类，4个二类)
# a 形容词
# ad 副形词
# an 名形词
# ag 形容词性语素
# al 形容词性惯用语
# 7. 区别词(1个一类，2个二类)
# b 区别词
# bl 区别词性惯用语
# 8. 状态词(1个一类)
# z 状态词
# 9. 代词(1个一类，4个二类，6个三类)
# r 代词
# rr 人称代词
# rz 指示代词
# rzt 时间指示代词
# rzs 处所指示代词
# rzv 谓词性指示代词
# ry 疑问代词
# ryt 时间疑问代词
# rys 处所疑问代词
# ryv 谓词性疑问代词
# rg 代词性语素
# 10. 数词(1个一类，1个二类)
# m 数词
# mq 数量词
# 11. 量词(1个一类，2个二类)
# q 量词
# qv 动量词
# qt 时量词
# 12. 副词(1个一类)
# d 副词
# 13. 介词(1个一类，2个二类)
# p 介词
# pba 介词“把”
# pbei 介词“被”
# 14. 连词(1个一类，1个二类)
# c 连词
# cc 并列连词
# 15. 助词(1个一类，15个二类)
# u 助词
# uzhe 着
# ule 了 喽
# uguo 过
# ude1 的 底
# ude2 地
# ude3 得
# usuo 所
# udeng 等 等等 云云
# uyy 一样 一般 似的 般
# udh 的话
# uls 来讲 来说 而言 说来
# uzhi 之
# ulian 连 （“连小学生都会”）
# 16. 叹词(1个一类)
# e 叹词
# 17. 语气词(1个一类)
# y 语气词(delete yg)
# 18. 拟声词(1个一类)
# o 拟声词
# 19. 前缀(1个一类)
# h 前缀
# 20. 后缀(1个一类)
# k 后缀
# 21. 字符串(1个一类，2个二类)
# x 字符串
# xx 非语素字
# xu 网址URL
# 22. 标点符号(1个一类，16个二类)
# w 标点符号
# wkz 左括号，全角：（ 〔 ［ ｛ 《 【 〖 〈 半角：( [ { <
# wky 右括号，全角：） 〕 ］ ｝ 》 】 〗 〉 半角： ) ] { >
# wyz 左引号，全角：“ ‘ 『
# wyy 右引号，全角：” ’ 』
# wj 句号，全角：。
# ww 问号，全角：？ 半角：?
# wt 叹号，全角：！ 半角：!
# wd 逗号，全角：， 半角：,
# wf 分号，全角：； 半角： ;
# wn 顿号，全角：、
# wm 冒号，全角：： 半角： :
# ws 省略号，全角：…… …
# wp 破折号，全角：—— －－ ——－ 半角：--- ----
# wb 百分号千分号，全角：％ ‰ 半角：%
# wh 单位符号，全角：￥ ＄ ￡ ° ℃ 半角：$
```

### 动态

```python
# 动态添加词典
jieba.add_word('极客大学')

# 动态删除词典
jieba.del_word('自定义词')

result = jieba.cut(string, cut_all=False)
print('动态添加: ' + '/'.join(list(result)))

print('=' * 40 )

string2 = '我们中出了一个叛徒'
result = jieba.cut(string2, cut_all=False)
print('错误分词: ' + '/'.join(list(result)))

print('=' * 40 )

# 关闭自动计算词频
result = jieba.cut(string2, HMM=False)
print('关闭词频: ' + '/'.join(list(result)))


print('=' * 40 )
# 调整分词，合并
jieba.suggest_freq('中出', True)

result = jieba.cut(string2, HMM=False)
print('分词合并: ' + '/'.join(list(result)))

print('=' * 40 )
# 调整词频，被分出来
string3 = '如果放到Post中将出错'
jieba.suggest_freq(('中','将'), True)
result = jieba.cut(string3, HMM=False)
print('分开分词: ' + '/'.join(list(result)))
```

### 并发

一般不用，重写算法，调试模型，jieba 主要用来测试模型

```python
import jieba
jieba.enable_parallel(4)
```

jieba github 上下载词库，很旧 7 年前

哈工大语料库比较准确

- 词频分析优化
- 移除低价值词语
- 句子的预处理：如中英文标点、数字等，用正则去掉
- 未登录词：HMM 模型，隐马尔可夫模型；Viterbi 算法。处理未登录词
- TF-IDF（词频-逆文件频率）：TF 在本地查找词频最高的，然后到词库中查词频最低的即DF低，IDF 就高；TF*IDF 倒序取最前面的 5 个

## wordcloud（词云）

### 分词取词

```python
import pandas as pd

def get_shorts():
    df = pd.read_csv('book_utf8.csv')
    # 调整格式
    df.columns = ['star', 'vote', 'shorts']
    star_to_number = {
        '力荐' : 5,
        '推荐' : 4,
        '还行' : 3,
        '较差' : 2,
        '很差' : 1
    }
    df['new_star'] = df['star'].map(star_to_number)

    # 查看数量
    # df.groupby('new_star').count()
    df2 = df[df['new_star'] == 3 ]
    # 取得评论内容
    return df2['shorts'].to_string()


import jieba.analyse
text = get_shorts()
stop_words=r'day0402/extra_dict/stop_words.txt'
jieba.analyse.set_stop_words(stop_words)
# 基于TF-IDF算法进行关键词抽取
tfidf = jieba.analyse.extract_tags(text,
    topK=10,                   # 权重最大的topK个关键词
    withWeight=False)
# >>> tfidf
# ['汤川', '东野', '短篇', '故事', '伽利略', '系列', '汤川学', '魔术', '神探', '扩写']
```

```sh
pip install wordcloud
```

### 生成词云

```python
import numpy as np
from wordcloud import WordCloud,STOPWORDS,ImageColorGenerator
from PIL import Image
from matplotlib import pyplot as plt
from matplotlib.pyplot import imread
import random


text_list = ['汤川', '东野', '短篇', '故事', '伽利略', '系列', '汤川学', '魔术', '神探', '扩写']
text_string = ','.join(text_list)

# 生成词云
wc = WordCloud(
    width = 600,      #默认宽度
    height = 200,     #默认高度
    margin = 2,       #边缘
    ranks_only = None,
    prefer_horizontal = 0.9,
    mask = None,      #背景图形,如果想根据图片绘制，则需要设置
    color_func = None,
    max_words = 200,  #显示最多的词汇量
    stopwords = None, #停止词设置，修正词云图时需要设置
    random_state = None,
    background_color = '#ffffff',#背景颜色设置，可以为具体颜色，比如：white或者16进制数值。
    font_step = 1,
    mode = 'RGB',
    regexp = None,
    collocations = True,
    normalize_plurals = True,
    contour_width = 0,
    colormap = 'viridis',#matplotlib色图，可以更改名称进而更改整体风格
    contour_color = 'Blues',
    repeat = False,
    scale = 2,
    min_font_size = 10,
    max_font_size = 200)

wc.generate_from_text(text_string)


# 显示图像
plt.imshow(wc, interpolation = 'bilinear')
plt.axis('off')
plt.tight_layout()
# 存储图像
wc.to_file('book.png')

plt.show()

# 中文乱码
# wordcloud.py P30
# FONT_PATH = os.environ.get('FONT_PATH', os.path.join(FILE, 'PingFang.ttc'))
```

### 美化

```python
import os
from os import path
import numpy as np
from wordcloud import WordCloud,STOPWORDS,ImageColorGenerator
from PIL import Image
from matplotlib import pyplot as plt
from matplotlib.pyplot import imread
import random

# 当前文件所在目录
dir = path.dirname(__file__)
# 获取文本text
text = open(path.join(dir, 'shopping.txt'),encoding='utf-8').read()

# 读取背景图片
background_Image = np.array(Image.open(path.join(dir, 'sp_background.jpg')))

# 提取背景图片颜色
img_colors = ImageColorGenerator(background_Image)

# 生成词云
wc = WordCloud(
  width = 600,      #默认宽度
  height = 200,     #默认高度
  margin = 2,       #边缘
  ranks_only = None,
  prefer_horizontal = 0.9,
  mask = background_Image,      #背景图形,如果想根据图片绘制，则需要设置
  color_func = None,
  max_words = 200,  #显示最多的词汇量
  stopwords = None, #停止词设置，修正词云图时需要设置
  random_state = None,
  background_color = '#ffffff',#背景颜色设置，可以为具体颜色，比如：white或者16进制数值。
  font_step = 1,
  mode = 'RGB',
  regexp = None,
  collocations = True,
  normalize_plurals = True,
  contour_width = 0,
  colormap = 'viridis',#matplotlib色图，可以更改名称进而更改整体风格
  contour_color = 'Blues',
  repeat = False,
  scale = 2,
  min_font_size = 10,
  max_font_size = 200)

wc.generate_from_text(text)

# 根据图片色设置背景色
wc.recolor(color_func = img_colors)

# 显示图像
plt.imshow(wc, interpolation = 'bilinear')
plt.axis('off')
plt.tight_layout()
# 存储图像
wc.to_file('love.png')

plt.show()
```

## smtplib

## pymysql

## 语义情感分析
