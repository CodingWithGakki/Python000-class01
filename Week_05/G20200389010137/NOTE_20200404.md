# 语意情感分析

自然语言处理：文字、对白、听说读写。庞大学科

语意情感分析：好坏二分类，多分类。是一种技术，应用到舆情分析

有一定的步骤

- 收集数据集
- 设计文本的表示模型（机器能读懂文字；怎么判断字的好坏，人 根据表情语气理解等，计算机要转换成数学描述）
- 选择文本的特征：提取关键词，关键词是正向还是负向
- 选择分类模型：决策树、贝叶斯分类器等非常多，效果不同

## 使用 snowNLP 进行语意分析

pip install snownlp

其使用电商的词库，多淘宝等比较准，对书评不准，需要重新训练模型

### 基本使用

```python
from snownlp import SnowNLP
text = '其实故事本来真的只值三星当初的中篇就足够了但是啊看到最后我又一次被东野叔的反战思想打动了所以就加多一星吧'
s = SnowNLP(text)

# 1 中文分词
s.words

# 2 词性标注
list(s.tags)

# 3 情感分析
s.sentiments                # 结果靠近 1 更正向；先判断准不准，不准要先训练模型

text2 = '这本书烂透了'
s2 = SnowNLP(text2)
s2.sentiments

# 4 拼音（使用 Trie 树算法）
s.pinyin

# 5 繁体转简体
text3 = '後面這些是繁體字'
s3 = SnowNLP(text3)
s3.han

# 6 提取关键字，建议用 jieba
s.keywords(limit=5)

# 7 信息衡量
s.tf # 词频越大越重要
s.idf # 包含此条的文档越少，n越小，idf越大，说明词条t越重要

# 8 训练
from snownlp import seg
seg.train('data.txt')
seg.save('seg.marshal')
# 修改snownlp/seg/__init__.py的 data_path 指向新的模型即可
```

### 语意分析

```python
import pandas as pd
from snownlp import SnowNLP

# 加载爬虫的原始评论数据
df = pd.read_csv('book_utf8.csv')
# 调整格式
df.columns = ['star', 'vote', 'shorts']
star_to_number = {
    '力荐' : 5,
    '推荐' : 4,
    '还行' : 3,
    '较差' : 2,
    '很差' : 1
}
df['new_star'] = df['star'].map(star_to_number)
# 用第一个评论做测试
first_line = df[df['new_star'] == 3].iloc[0]
text = first_line['shorts']
s = SnowNLP(text)
print(f'情感倾向: {s.sentiments} , 文本内容: {text}')

# 封装一个情感分析的函数
def _sentiment(text):
    s = SnowNLP(text)
    return s.sentiments

df["sentiment"] = df.shorts.apply(_sentiment)
# 查看结果
df.head()
# 分析平均值
df.sentiment.mean()




# 训练模型
# from snownlp import sentiment
# sentiment.train('neg.txt','pos.txt')  # pos.txt 正向模型；neg.txt 负向模型
# sentiment.save('sentiment.marshal')   # 保存模型
```

### 特殊字处理

变体字、中英文混合、中文拼音、简繁、同音字、形近字、拆字、特殊字符、emoji。github 词库能解决 70-80%的问题

china beijing
japan tokyo
？ tokyo

距离

## 词向量

- 标量：直线中一个点
- 向量：平面中的一条直线
- 矩阵：平面基，x，y 轴，二维
- 张量：三维，矩阵堆叠成空间

```python
import numpy as np

# 标量
a = 1
print(a)

# 向量
v1 = np.array([1, 3])
print(v1)
print(v1.shape)

# 矩阵
m1 = np.array([[1, 0], [0, 1]])
print(m1)
print(m1.shape)

# 向量减法
v1 = np.array([1, 0])
v2 = np.array([0, 1])
print(v1 + v2)
v3 = np.array([1, 1])
print(v3 - v1)

# 曼哈顿距离：先 x 轴直行，再 y 轴直行
v1 = np.array([1, 1])
v2 = np.array([2, 2])
from numpy import linalg
print(linalg.norm(v2 - v1, 1))      # 1：曼哈顿距离；2：欧几里得距离

# 欧几里得距离：直线距离
print(linalg.norm(v2 - v1, 2))      # 1：曼哈顿距离；2：欧几里得距离

# 矩阵，用 GPU 计算
m1 = np.array([[1, 2], [2, 3]])
print(m1)
print(m1.shape)

# 矩阵与标量运算
print(m1 + 1)

# 向量的维度(2维)
print(v2.shape)

# 向量的维度(3维)
v3 = np.array([1, 1, 1])
print(v3.shape)

# 矩阵相加
m1 = np.array([[1, 0], [0, 1]])
m2 = np.array([[1, 1], [1, 1]])
print(m1.shape, m2.shape)
print(m1 + m2)

# 矩阵形状不一致会报错
m3 = np.array([[1,], [0,]])
m4 = np.array([[1,], [1,], [2,]])
print(m3.shape, m4.shape)
print(m3 + m4)
```

### pytorch（深度学习入门）

```python
import torch
import torchtext
from torchtext import vocab
# 预先训练好的此向量
gv = torchtext.vocab.GloVe(name='6B', dim=50)   # 加载库
# 40万个词，50个维度
len(gv.vectors), gv.vectors.shape
# 获得单词的在Glove词向量中的索引(坐标)
gv.stoi['tokyo']        # s to i
# 查看tokyo的词向量
gv.vectors[1363]
# 可以把坐标映射回单词
gv.itos[1363]           # i to s

# 把tokyo转换成词向量
def get_wv(word):
    return gv.vectors[gv.stoi[word]]
get_wv('tokyo')

# 找到距离最近的10个单词
def sim_10(word, n=10):
    all_dists = [(gv.itos[i], torch.dist(word, w)) for i, w in enumerate(gv.vectors)]
    return sorted(all_dists, key=lambda t: t[1])[:n]
sim_10(get_wv('tokyo'))

def analogy(w1, w2, w3, n=5, filter_given=True):
    print(f'[ {w1} : {w2} :: {w3} : ? ]')

    # w2 - w1 = w4 - w3 变成下面算法。china-beijing=xx-tokoy
    # w2 - w1 + w3 = w4
    closest_words = sim_10(get_wv(w2) - get_wv(w1) + get_wv(w3))    # sim_10 取近似，无法精确

    # 过滤防止输入参数出现在结果中
    if filter_given:
        closest_words = [t for t in closest_words if t[0] not in [w1, w2, w3]]
    print(closest_words[:2])

analogy('beijing', 'china', 'tokyo')
# [ beijing : china :: tokyo : ? ]
# [('japan', tensor(2.7869)), ('japanese', tensor(3.6377))]
```

底层用 cuda 技术，GPU 主要用来做多边形计算，这里用于矩阵计算

## cv（计算机视觉）

- opencv
- pytesseract，多字母和数字的相当准确

